%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Dem≈°ar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}

\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{Machine Learning for Data Science, 2025}

% Article title
\PaperTitle{
Evaluating Prediction Models: Performance, Error Analysis, and Distribution Shifts
}

% Authors (student competitors) and their info
\Authors{Teodora Taleska}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
%\flushbottom
%
\raggedbottom

% Print the title and abstract box
\maketitle

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}

This report evaluates models for predicting basketball shot types. We compare a \textit{Baseline Classifier}, \textit{Logistic Regression}, and \textit{Random Forest} optimized through standard and nested cross-validation. Random Forest achieved the highest accuracy and lowest log-loss.

We also examined the relationship between shot distance and prediction errors, finding a weak negative correlation using the \textit{Spearman rank coefficient}, indicating errors slightly decrease with distance but are not strongly dependent on it.

Finally, adjusting for the true competition type distribution increased log-loss significantly while accuracy remained unchanged, suggesting that while predictions were correct at the same rate, their confidence degraded. This underscores the importance of evaluating models under real-world data distributions.

%------------------------------------------------
\section*{Part 1: Model Evaluation and Comparison}

\subsection*{Methodology}

\subsubsection*{Dataset and Problem Statement}
The dataset consists of basketball shot data with the goal of predicting the \textit{ShotType} (6 categories) using all other variables. The dataset is assumed to be a representative sample of the data-generating process.

\subsubsection*{Chosen Models}
Three models were compared: a \textit{Baseline Classifier} predicting the most frequent class, \textit{Logistic Regression} for multi-class classification, and \textit{Random Forest}, an ensemble model sensitive to hyperparameters such as the number of trees. Random Forest was chosen for its strong performance and sensitivity to hyperparameter tuning.

\subsubsection*{Evaluation Method}
The models were evaluated using \textit{5-fold cross-validation} with a 70-30 train-test split. The dataset contains 5024 instances, resulting in approximately 3500 training instances. With 5 folds, each fold contained around 700 instances, ensuring a balance between robust performance estimation and computational efficiency. For Random Forest, two hyperparameter tuning strategies were used: \textit{Standard Cross-Validation}, where hyperparameters were tuned on the training folds, and \textit{Nested Cross-Validation}, where an outer loop evaluated performance and an inner loop tuned hyperparameters.

\subsubsection*{Metrics}
The models were evaluated using \textit{Accuracy}, the proportion of correctly predicted instances, and \textit{Log-Loss}, which measures the confidence of predicted probabilities for multi-class classification. Accuracy is defined as:

\[
\text{Accuracy} = \frac{1}{N} \sum_{i=1}^N \mathbb{I}(y_i = \hat{y}_i),
\]

where \( N \) is the total number of instances, \( y_i \) is the true label, and \( \hat{y}_i \) is the predicted label. Log-Loss is defined as:

\[
\text{Log-Loss} = -\frac{1}{N} \sum_{i=1}^N \sum_{c=1}^C y_{i,c} \log(p_{i,c}),
\]

where \( y_{i,c} \) is 1 if the true label of instance \( i \) is class \( c \), and \( p_{i,c} \) is the predicted probability of class \( c \).

\subsection*{Results}

% TODO: Present results and compare model performance.

%------------------------------------------------

\section*{Part 2: Error Analysis and Adjusting for True Distribution}

\subsection*{Assessing the Influence of Shot Distance on Prediction Errors}

% TODO: Explain the approach (Spearman correlation).

\subsubsection*{Results}

% TODO: Summarize findings (weak correlation but not a strong dependency).


\subsection*{Adjusting for True Distribution}

% TODO: Explain reweighting approach.

\subsubsection*{Results}


% TODO: Report performance changes (log-loss increase, accuracy stability).
% TODO: Interpret the meaning of these changes.





%------------------------------------------------

%\section*{Discussion}


%------------------------------------------------


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}