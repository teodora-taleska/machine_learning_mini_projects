{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Bayesian Inference",
   "id": "87df71bb53ee9986"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T10:34:48.135904Z",
     "start_time": "2025-08-27T10:34:48.121037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.integrate import quad\n",
    "from scipy.stats import beta, binomtest"
   ],
   "id": "89492c45bf45df34",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bayesian computation",
   "id": "be33b629c9cf1a87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Toy density (target distribution we want to approximate)\n",
    "# -----------------------------------------------------------------------------\n",
    "# In the R code, prop_f was a skew-normal-like distribution.\n",
    "# We'll define it here directly.\n",
    "\n",
    "def prop_f(x):\n",
    "    # skew-normal-like density: proportional to normal * cdf(normal)\n",
    "    return 2 * norm.pdf(x, loc=0, scale=4) * norm.cdf(5 * x, loc=0, scale=4)\n",
    "\n",
    "x = np.linspace(-5, 15, 1000)\n",
    "\n",
    "plt.plot(x, prop_f(x), label=\"Target density\")\n",
    "plt.ylim(0, 0.3)\n",
    "plt.title(\"Toy target density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Metropolis-Hastings (MCMC)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def markov_transition(x):\n",
    "    # Propose new sample from a uniform window around x\n",
    "    x_new = np.random.uniform(x - 0.1, x + 0.1)\n",
    "    \n",
    "    # Acceptance probability (log version for stability)\n",
    "    p = np.exp(np.log(prop_f(x_new)) - np.log(prop_f(x)))\n",
    "    \n",
    "    # Accept/reject\n",
    "    if np.random.rand() > p:\n",
    "        x_new = x\n",
    "    return x_new\n",
    "\n",
    "np.random.seed(0)\n",
    "m = 100000  # number of iterations\n",
    "y = np.zeros(m + 1)\n",
    "\n",
    "# Run the Markov chain\n",
    "for i in range(m):\n",
    "    y[i+1] = markov_transition(y[i])\n",
    "\n",
    "# Histogram of samples vs. target density\n",
    "counts, bins = np.histogram(y, bins=50, density=True)\n",
    "plt.plot(x, prop_f(x), label=\"Target density\")\n",
    "plt.step(bins[:-1], counts, where=\"post\", color=\"green\", label=\"MCMC histogram\")\n",
    "plt.ylim(0, 0.3)\n",
    "plt.legend()\n",
    "plt.title(\"Metropolis-Hastings sampling\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Laplace approximation\n",
    "# -----------------------------------------------------------------------------\n",
    "# Laplace approximation fits a Gaussian around the mode of the log density.\n",
    "\n",
    "# Objective: negative log of target density\n",
    "fn = lambda z: -np.log(prop_f(z))\n",
    "\n",
    "res = minimize(fn, x0=[0], method=\"L-BFGS-B\", hess=True)\n",
    "mu = res.x[0]\n",
    "# Extract approximate variance from Hessian (second derivative)\n",
    "# res.hess_inv is an approximation to the inverse Hessian.\n",
    "s2 = res.hess_inv.todense()[0,0]\n",
    "\n",
    "plt.plot(x, prop_f(x), label=\"Target density\")\n",
    "plt.plot(x, norm.pdf(x, mu, np.sqrt(s2)), \"r\", label=\"Laplace approx\")\n",
    "plt.ylim(0, 0.3)\n",
    "plt.legend()\n",
    "plt.title(\"Laplace approximation\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Mean-Field Variational Inference (Gaussian variational family)\n",
    "# -----------------------------------------------------------------------------\n",
    "# We maximize the ELBO: E_q[log p(x)] - E_q[log q(x)]\n",
    "\n",
    "# ELBO function to optimize\n",
    "def elbo(params):\n",
    "    mu, ss = params  # variational mean and std dev\n",
    "    \n",
    "    # log target density\n",
    "    logp = lambda z: np.log(prop_f(z))\n",
    "    \n",
    "    # integrand for E_q[log q(x)]\n",
    "    def f1(z):\n",
    "        qz = norm.pdf(z, mu, ss)\n",
    "        return qz * norm.logpdf(z, mu, ss)\n",
    "    \n",
    "    # integrand for E_q[log p(x)]\n",
    "    def f2(z):\n",
    "        qz = norm.pdf(z, mu, ss)\n",
    "        return qz * logp(z)\n",
    "    \n",
    "    # Numerical integration over finite domain (approximation)\n",
    "    eq_logp, _ = quad(f2, -5, 10)\n",
    "    eq_logq, _ = quad(f1, -5, 10)\n",
    "    \n",
    "    return eq_logp - eq_logq\n",
    "\n",
    "# Optimize ELBO (maximize)\n",
    "res_vi = minimize(lambda v: -elbo(v), x0=[0,1], bounds=[(None,None),(1e-4,None)], method=\"L-BFGS-B\")\n",
    "mu_vi, ss_vi = res_vi.x\n",
    "\n",
    "plt.plot(x, prop_f(x), label=\"Target density\")\n",
    "plt.plot(x, norm.pdf(x, mu_vi, ss_vi), \"r\", label=\"VI approx\")\n",
    "plt.ylim(0, 0.3)\n",
    "plt.legend()\n",
    "plt.title(\"Mean-Field Variational Inference\")\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Example\n",
    "three perspectives on estimating a binomial proportion: frequentist MLE with asymptotic CI, exact binomial CI, and Bayesian Beta posterior credible interval"
   ],
   "id": "59d5c5808e286de7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T10:36:39.361973Z",
     "start_time": "2025-08-27T10:36:39.337899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "n = 1318   # total trials\n",
    "k = 41     # number of successes\n",
    "\n",
    "# Data vector not strictly needed in Python, but equivalent to R's rep()\n",
    "y = np.concatenate([np.ones(k), np.zeros(n - k)])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MLE (sample proportion) and asymptotic normal CI\n",
    "# -----------------------------------------------------------------------------\n",
    "p_est = np.mean(y) # MLE estimator for the mean\n",
    "SE = np.std(y, ddof=1) / np.sqrt(n)  # standard error using sample sd\n",
    "\n",
    "# Rough 95% CI with Â± 2*SE rule of thumb\n",
    "print(f\"Rough 95% CI: {p_est - 2*SE:.4f} - {p_est + 2*SE:.4f}\")\n",
    "print(\"This CI will contain the true value of the parameter 95% of the time in repeated sampling.\")\n",
    "# but saying that the true value has a 95% probability of being in this interval is incorrect.\n",
    "print(\"------------------------------\")\n",
    "\n",
    "print(f\"MLE: p = {p_est:.4f} (SE = {SE:.4f})\")\n",
    "print(f\"95% CI (asymptotic): [{p_est - 1.96*SE:.4f}, {p_est + 1.96*SE:.4f}]\")\n",
    "\n",
    "# Exact CI via Clopper-Pearson (Beta quantiles)\n",
    "ci_low = beta.ppf(0.025, k + 1, n - k + 1)\n",
    "ci_high = beta.ppf(0.975, k + 1, n - k + 1)\n",
    "print(f\"95% CI (exact / Clopper-Pearson): [{ci_low:.4f}, {ci_high:.4f}]\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Binomial test\n",
    "# -----------------------------------------------------------------------------\n",
    "# Two-sided binomial test for H0: p = 0.5 (default)\n",
    "# Can change 'p' argument for different nulls\n",
    "res = binomtest(k, n, p=0.5, alternative='two-sided')\n",
    "print(\"\\nBinomial test result:\")\n",
    "print(res)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Bayesian posterior interval\n",
    "# -----------------------------------------------------------------------------\n",
    "# Uniform prior Beta(1,1) -> posterior Beta(k+1, n-k+1)\n",
    "ci_low_bayes = beta.ppf(0.025, k + 1, n - k + 1)\n",
    "ci_high_bayes = beta.ppf(0.975, k + 1, n - k + 1)\n",
    "print(f\"Bayesian 95% credible interval (uniform prior): {ci_low_bayes:.4f} - {ci_high_bayes:.4f}\")\n"
   ],
   "id": "9e25ebfa8dbb28fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rough 95% CI: 0.0215 - 0.0407\n",
      "MLE: p = 0.0311 (SE = 0.0048)\n",
      "95% CI (asymptotic): [0.0217, 0.0405]\n",
      "95% CI (exact / Clopper-Pearson): [0.0230, 0.0419]\n",
      "\n",
      "Binomial test result:\n",
      "BinomTestResult(k=41, n=1318, alternative='two-sided', statistic=0.03110773899848255, pvalue=4.75054e-319)\n",
      "Bayesian 95% credible interval (uniform prior): 0.0230 - 0.0419\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
